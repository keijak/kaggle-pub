{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/Users/keiji/work/kaggle/sales1c/'\n",
    "\n",
    "def read(file_name):\n",
    "    pickle = DATA_DIR + file_name + '.pickle'\n",
    "    if os.path.exists(pickle):\n",
    "        return pd.read_pickle(pickle)\n",
    "    df = pd.read_csv(DATA_DIR + file_name)\n",
    "    df.to_pickle(pickle)\n",
    "    return df\n",
    "\n",
    "df_icats = read('item_categories.csv')\n",
    "df_items = read('items.csv')\n",
    "df_shops = read('shops.csv')\n",
    "df_test = read('test.csv.gz')\n",
    "df_sales = read('sales_train.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outliers.\n",
    "# I'm not sure if this really matters.\n",
    "df_sales = df_sales[df_sales.item_price < 60000]\n",
    "df_sales = df_sales[df_sales.item_cnt_day < 700]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales['item_sales'] = df_sales.item_price * df_sales.item_cnt_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_shop_id = df_sales[['date_block_num', 'shop_id']].drop_duplicates().reset_index(drop=True)\n",
    "unique_item_id = df_sales[['date_block_num', 'item_id']].drop_duplicates().reset_index(drop=True)\n",
    "df_train = (\n",
    "    pd.DataFrame({'date_block_num': np.arange(34)})\n",
    "    .merge(unique_shop_id, how='left', on='date_block_num')\n",
    "    .merge(unique_item_id, how='left', on='date_block_num')\n",
    "    .merge(df_items[['item_id', 'item_category_id']], how='left', on='item_id')\n",
    "    .sort_values(by=['date_block_num', 'shop_id', 'item_id'])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_train.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = (\n",
    "    df_sales.groupby(['date_block_num', 'shop_id', 'item_id'])\n",
    "    .agg({\n",
    "        'item_cnt_day': 'sum',\n",
    "        'item_price': 'median',\n",
    "        'item_sales': 'sum',\n",
    "    })\n",
    "    .reset_index()\n",
    "    .rename(columns={\n",
    "        'item_cnt_day': 'item_cnt_month',\n",
    "        'item_price': 'median_price',\n",
    "        'item_sales': 'item_sales_month',\n",
    "    })\n",
    ")\n",
    "df_train = df_train.merge(df_agg, how='left', on=['date_block_num', 'shop_id', 'item_id'])\n",
    "df_train.item_cnt_month.fillna(0.0, inplace=True)\n",
    "df_train.item_sales_month.fillna(0.0, inplace=True)\n",
    "df_train.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = (\n",
    "    df_train.groupby(['date_block_num', 'shop_id', 'item_category_id'])\n",
    "    .agg({\n",
    "        'item_cnt_month': 'sum',\n",
    "        'item_sales_month': 'sum',\n",
    "    })\n",
    "    .reset_index()\n",
    "    .rename(columns={\n",
    "        'item_cnt_month': 'icat_cnt_month',\n",
    "        'item_sales_month': 'icat_sales_month',\n",
    "    })\n",
    ")\n",
    "df_train = df_train.merge(df_agg, how='left', on=['date_block_num', 'shop_id', 'item_category_id'])\n",
    "df_train.icat_cnt_month.fillna(0.0, inplace=True)\n",
    "df_train.icat_sales_month.fillna(0.0, inplace=True)\n",
    "df_train.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = (\n",
    "    df_sales.groupby(['date_block_num', 'item_id'])\n",
    "    .agg({\n",
    "        'item_cnt_day': 'sum',\n",
    "        'item_price': 'median',\n",
    "        'item_sales': 'sum',\n",
    "    })\n",
    "    .reset_index()\n",
    "    .rename(columns={\n",
    "        'item_cnt_day': 'item_cnt_month_allshops',\n",
    "        'item_price': 'median_price_allshops',\n",
    "        'item_sales': 'item_sales_month_allshops',\n",
    "    })\n",
    ")\n",
    "df_train = df_train.merge(df_agg, how='left', on=['date_block_num', 'item_id'])\n",
    "df_train.median_price.fillna(df_train.median_price_allshops, inplace=True)\n",
    "df_train.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = (\n",
    "    df_train.groupby(['date_block_num', 'item_category_id'])\n",
    "    .agg({\n",
    "        'item_cnt_month_allshops': 'sum',\n",
    "        'item_sales_month_allshops': 'sum',\n",
    "    })\n",
    "    .reset_index()\n",
    "    .rename(columns={\n",
    "        'item_cnt_month_allshops': 'icat_cnt_month_allshops',\n",
    "        'item_sales_month_allshops': 'icat_sales_month_allshops',\n",
    "    })\n",
    ")\n",
    "df_train = df_train.merge(df_agg, how='left', on=['date_block_num', 'item_category_id'])\n",
    "df_train.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in [['shop_id'], ['item_category_id'], ['shop_id', 'item_category_id']]:\n",
    "    print('processing:', cols)\n",
    "    df_agg = (\n",
    "        df_train.groupby(['date_block_num'] + cols)\n",
    "        .item_cnt_month\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'item_cnt_month': 'target_enc#' + ('|'.join(cols))})\n",
    "    )\n",
    "    df_train = df_train.merge(df_agg, how='left', on=['date_block_num']+cols)\n",
    "    \n",
    "    agecol = 'age#' + ('|'.join(cols))\n",
    "    df_agg = (\n",
    "        df_train.groupby(cols)\n",
    "        .date_block_num\n",
    "        .min()\n",
    "        .reset_index()\n",
    "        .rename(columns={'date_block_num': agecol})\n",
    "    )\n",
    "    df_train = df_train.merge(df_agg, how='left', on=cols)\n",
    "    df_train[agecol] = df_train.date_block_num - df_train[agecol]\n",
    "\n",
    "df_train.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thsese signals are not useful...\n",
    "\n",
    "#item_cnt_ratio_by_shop = df_train.item_cnt_month / df_train.item_cnt_month_allshops\n",
    "#item_cnt_ratio_by_shop[np.abs(df_train.item_cnt_month) > df_train.item_cnt_month_allshops] = np.nan\n",
    "#df_train['item_cnt_month_by_shop'] = (df_train.item_cnt_month_allshops * item_cnt_ratio_by_shop.mean()).fillna(0)\n",
    "\n",
    "#item_cnt_ratio_by_icat = df_train.item_cnt_month / df_train.icat_cnt_month\n",
    "#item_cnt_ratio_by_icat[np.abs(df_train.item_cnt_month) > df_train.icat_cnt_month] = np.nan\n",
    "#df_train['item_cnt_month_by_icat'] = (df_train.icat_cnt_month * item_cnt_ratio_by_icat.mean()).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def rename_column(index, col_name):\n",
    "    if col_name in ['shop_id', 'item_id']:\n",
    "        return col_name\n",
    "    return '{}_{}'.format(col_name, index)\n",
    "    \n",
    "def make_features(label_block):\n",
    "    label_df = df_train.loc[df_train.date_block_num == label_block, ['shop_id', 'item_id', 'item_cnt_month']].reset_index(drop=True)\n",
    "    y = label_df.item_cnt_month#.map(lambda x: max(0, min(40, x)))\n",
    "    return make_features2(label_block, label_df), y\n",
    "\n",
    "def make_features2(label_block, label_df):\n",
    "    X = label_df[['shop_id', 'item_id']]\n",
    "    for i in [1,2,12,13]:\n",
    "        feature_block = label_block - i\n",
    "        dff = (\n",
    "            df_train.loc[df_train.date_block_num == feature_block]\n",
    "            .drop(['date_block_num', 'item_category_id'], axis=1)\n",
    "            .rename(partial(rename_column, i), axis='columns')\n",
    "        )\n",
    "        X = X.merge(dff, how='left', on=['shop_id', 'item_id'])\n",
    "\n",
    "    delta_features = ['item_cnt_month', 'item_cnt_month_allshops',\n",
    "                      'item_sales_month', 'item_sales_month_allshops',\n",
    "                      'icat_cnt_month', 'icat_cnt_month_allshops']\n",
    "    for prefix in delta_features:\n",
    "        for i in [1, 12]:\n",
    "            X['{}_DELTA{}'.format(prefix, i)] = X['{}_{}'.format(prefix,i)].fillna(0) - X['{}_{}'.format(prefix,i+1)].fillna(0)\n",
    "            # 1 month ago + delta\n",
    "            X['{}_INTERPOLATE{}'.format(prefix, i)] = X['{}_1'.format(prefix)].fillna(0) + X['{}_DELTA{}'.format(prefix, i)].fillna(0)\n",
    "\n",
    "    columns_to_drop = (\n",
    "        ['shop_id', 'item_id']\n",
    "        + [col for col in X.columns\n",
    "           if any([col.endswith('_{}'.format(i)) for i in [2,12,13]])])\n",
    "    X.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    X.fillna(0, inplace=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <32: TUNING\n",
    "# =32: STACKING TUNING\n",
    "# =33: FINAL VALIDATION\n",
    "# =34: TEST\n",
    "PREDICTION_BLOCK = 34\n",
    "TRAINING_BLOCK_RANGE = range(14, PREDICTION_BLOCK)\n",
    "\n",
    "def is_tuning():\n",
    "    return PREDICTION_BLOCK < 33\n",
    "def is_validation():\n",
    "    return PREDICTION_BLOCK < 34\n",
    "def is_test():\n",
    "    return PREDICTION_BLOCK == 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = []\n",
    "ys = []\n",
    "for label_block in TRAINING_BLOCK_RANGE:\n",
    "    X, y = make_features(label_block)\n",
    "    Xs.append(X)\n",
    "    ys.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat(Xs, sort=False)\n",
    "y_train = pd.concat(ys, sort=False)\n",
    "X_train.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_validation():\n",
    "    X_val, y_val = make_features(PREDICTION_BLOCK)\n",
    "    X_combined = pd.concat([X_train, X_val], sort=False)\n",
    "else:\n",
    "    X_test = make_features2(PREDICTION_BLOCK, df_test)\n",
    "    X_combined = pd.concat([X_train, X_test], sort=False)\n",
    "\n",
    "scaler = StandardScaler(copy=False)\n",
    "scaler.fit_transform(X_combined.values)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_combined.values)\n",
    "\n",
    "def transform(X):\n",
    "    pca_features = pca.transform(X.values)\n",
    "    X['pca0'] = pca_features[:,0]\n",
    "    X['pca1'] = pca_features[:,1]\n",
    "\n",
    "transform(X_train)\n",
    "if is_validation():\n",
    "    transform(X_val)\n",
    "else:\n",
    "    transform(X_test)\n",
    "\n",
    "X_train.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(13,13))\n",
    "sns.heatmap(X_train.corr(), cmap='coolwarm', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_tuning():\n",
    "    X_train.to_pickle(DATA_DIR + 'X_train.pickle')\n",
    "    y_train.to_pickle(DATA_DIR + 'y_train.pickle')\n",
    "    X_val.to_pickle(DATA_DIR + 'X_val.pickle')\n",
    "    y_val.to_pickle(DATA_DIR + 'y_val.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params={'num_leaves': 90, 'reg_lambda': 13.220339591026585, 'reg_alpha': 29.630695197981943, 'min_data_in_leaf': 12,\n",
    "        'colsample_bytree': 0.9555622996470243, 'learning_rate': 0.059291081728894066, 'max_depth': 8, 'subsample': 0.910633831712105}\n",
    "\n",
    "reg = LGBMRegressor(\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    n_estimators=500,\n",
    "    **params,\n",
    ")\n",
    "\n",
    "if is_validation():\n",
    "    reg.fit(X_train, y_train, eval_metric='mse', early_stopping_rounds=200, eval_set=[(X_val, y_val)], verbose=True)\n",
    "else:\n",
    "    reg.fit(X_train, y_train, eval_metric='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame({'feature': X_train.columns, 'importance': reg.feature_importances_})\n",
    "feature_importances.sort_values(by='importance', ascending=True, inplace=True)\n",
    "feature_importances.plot(x='feature', y='importance', kind='barh', sort_columns=True, figsize=(10,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(s):\n",
    "    return s.map(lambda x: max(0.0, min(20.0, x)))\n",
    "\n",
    "def rmse(x, y):\n",
    "    return math.sqrt(mean_squared_error(clip(x), clip(y)))\n",
    "\n",
    "if is_validation():\n",
    "    y_pred = pd.Series(reg.predict(X_val))\n",
    "    print('validation RMSE score:', rmse(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = pd.Series(reg.predict(X_train))\n",
    "print('training RMSE score', rmse(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if is_test():\n",
    "    y_test_pred = clip(pd.Series(reg.predict(X_test)))\n",
    "    y_test_pred_df = pd.DataFrame(y_test_pred)\n",
    "    y_test_pred_df.info()\n",
    "    y_test_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_test():\n",
    "    df_submit = pd.DataFrame(df_test.loc[:,'ID'])\n",
    "    df_submit['item_cnt_month'] = y_test_pred\n",
    "    df_submit.to_csv('lgbm_tuned6.csv', index=False)\n",
    "    df_submit.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
